{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Work Done**\n",
    "- Able to compare cosine distance and euclidian distance between two sentences using embeddings generated by MURIL\n",
    "- Used nltk and wordnet to find words close in meaning for a given input word (not using with the model at the moment)\n",
    "- Used spacy to randomly select Adjectives,Nouns,Verbs from a given sentence\n",
    "- Using nlpaug we can replace either the entire sentence with an augmented form or replace a random word with a \"similar\" one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Used Google Translate model to translate between Hindi and English to test which Idioms when translated would result in their actual meaning.**\n",
    "- Currently continuing further experimentation related to this idea.\n",
    "\n",
    "\n",
    "**Idioms which work**\n",
    "- in hot water\n",
    "- both go hand in hand\n",
    "- dont sweat it\n",
    "- beat around the bush\n",
    "- call it a day\n",
    "- easy as pie\n",
    "- do not sweat it, dont sweat it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_text as text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import bert\n",
    "from bert import bert_tokenization\n",
    "import numpy as np\n",
    "from scipy.spatial import distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function definition\n",
    "def get_model(model_url, max_seq_length):\n",
    "  inputs = dict(\n",
    "    input_word_ids=tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32),\n",
    "    input_mask=tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32),\n",
    "    input_type_ids=tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32),\n",
    "    ) \n",
    "  muril_layer = hub.KerasLayer(model_url, trainable=True)\n",
    "  outputs = muril_layer(inputs)\n",
    "  assert 'sequence_output' in outputs\n",
    "  assert 'pooled_output' in outputs\n",
    "  assert 'encoder_outputs' in outputs\n",
    "  assert 'default' in outputs\n",
    "  return tf.keras.Model(inputs=inputs,outputs=outputs[\"pooled_output\"]), muril_layer\n",
    "#function call\n",
    "max_seq_length = 128\n",
    "\n",
    "muril_model, muril_layer = get_model(\n",
    "    model_url=\"https://tfhub.dev/google/MuRIL/1\", max_seq_length=max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converts input into BERT-acceptable format (preprocessing)\n",
    "vocab_file = muril_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "do_lower_case = muril_layer.resolved_object.do_lower_case.numpy()\n",
    "tokenizer = bert_tokenization.FullTokenizer(vocab_file, do_lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the 3 types of embeddings in BERT\n",
    "def create_input(input_strings, tokenizer, max_seq_length):\n",
    "  input_ids_all, input_mask_all, input_type_ids_all = [], [], []\n",
    "  for input_string in input_strings:\n",
    "    input_tokens = [\"[CLS]\"] + tokenizer.tokenize(input_string) + [\"[SEP]\"]\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(input_tokens)\n",
    "    sequence_length = min(len(input_ids), max_seq_length)\n",
    "    if len(input_ids) >= max_seq_length:\n",
    "      input_ids = input_ids[:max_seq_length]\n",
    "    else:\n",
    "      input_ids = input_ids + [0] * (max_seq_length - len(input_ids))\n",
    "    input_mask = [1] * sequence_length + [0] * (max_seq_length - sequence_length)\n",
    "    input_ids_all.append(input_ids)\n",
    "    input_mask_all.append(input_mask)\n",
    "    input_type_ids_all.append([0] * max_seq_length)\n",
    "  return np.array(input_ids_all), np.array(input_mask_all), np.array(input_type_ids_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(input_text):\n",
    "  input_ids, input_mask, input_type_ids = create_input(input_text,tokenizer,max_seq_length)\n",
    "  inputs = dict(\n",
    "      input_word_ids=input_ids,\n",
    "      input_mask=input_mask,\n",
    "      input_type_ids=input_type_ids,\n",
    "  )\n",
    "  return muril_model(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nlpaug.augmenter.word as naw\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_random(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    words = [token.text for token in doc if token.pos_ in ['VERB','ADJ','NOUN']]\n",
    "    if(len(words)==0):\n",
    "        return doc[0].text\n",
    "    random_word_index = random.randrange(len(words))  # Generate a random index\n",
    "    random_word = words[random_word_index]\n",
    "    return random_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug = naw.SynonymAug(\n",
    "    aug_src='wordnet',\n",
    "    model_path=None,\n",
    "    name='Synonym_Aug',\n",
    "    aug_min=1,\n",
    "    aug_max=10,\n",
    "    aug_p=0.1,  # Adjust this value for closer synonyms\n",
    "    lang='eng',\n",
    "    stopwords=None,\n",
    "    tokenizer=None,\n",
    "    reverse_tokenizer=None,\n",
    "    stopwords_regex=None,\n",
    "    force_reload=False,\n",
    "    verbose=0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_w_aug(sentence):\n",
    "    word=select_random(sentence)\n",
    "    word_aug = aug.augment(word)\n",
    "    l=sentence.split()\n",
    "    l[l.index(word)]=word_aug[0]\n",
    "    l=' '.join(l)\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclid_dist(embeddings,x,y):\n",
    "    dst_1 = distance.euclidean(np.array(embeddings[x]), np.array(embeddings[y]))\n",
    "    print(\"Euclidian Distance between '{}' & '{}' is {}\".format(sentences[x],sentences[y],dst_1))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "def cosine_dist(embeddings,x,y):\n",
    "    # Assuming embeddings1 and embeddings2 are the embeddings of two sentences\n",
    "    cosine_similarity1 = 1 - cosine(embeddings[x], embeddings[y])\n",
    "    print(\"Cosine Similarity for '{}' & '{}' is {}\".format(sentences[x],sentences[y],cosine_similarity1))\n",
    "    cosine_distance1 = 1 - cosine_similarity1\n",
    "    print(\"Cosine Distance between '{}' & '{}' is {}\".format(sentences[x],sentences[y],cosine_distance1))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in ADP\n",
      "hot ADJ\n",
      "water NOUN\n"
     ]
    }
   ],
   "source": [
    "doc=nlp(idiom)\n",
    "for d in doc:\n",
    "    print(d.text,d.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Euclidian Distance between 'dont worry about it' & 'dont sweat it' is 0.0038874142337590456\n",
      "\n",
      "Euclidian Distance between 'dont worry about it' & 'dont perspiration it' is 0.00786435417830944\n",
      "\n",
      "Cosine Similarity for 'dont worry about it' & 'dont sweat it' is 0.9999198317527771\n",
      "Cosine Distance between 'dont worry about it' & 'dont sweat it' is 8.016824722290039e-05\n",
      "\n",
      "Cosine Similarity for 'dont worry about it' & 'dont perspiration it' is 0.9996719360351562\n",
      "Cosine Distance between 'dont worry about it' & 'dont perspiration it' is 0.00032806396484375\n",
      "\n"
     ]
    }
   ],
   "source": [
    "euclid_dist(embeddings,0,1)\n",
    "euclid_dist(embeddings,0,2)\n",
    "cosine_dist(embeddings,0,1)\n",
    "cosine_dist(embeddings,0,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from translate import Translator\n",
    "\n",
    "translator = Translator(to_lang=\"hi\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "real=\"in trouble\"\n",
    "idiom=\"in hot water\"\n",
    "sentences = [real,idiom,\"in hot coffee\"]\n",
    "# embeddings = encode(sentences)\n",
    "# embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "मुसीबत में in trouble\n",
      "गर्म पानी में in hot water\n",
      "हॉट कॉफ़ी में in hot coffee\n"
     ]
    }
   ],
   "source": [
    "for sentence in sentences:\n",
    "    translation = translator.translate(sentence)\n",
    "    print(translation, sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = nlp(\"This is a sentence.\")\n",
    "sentence2 = nlp(\"This is another sentence.\")\n",
    "\n",
    "similarity = sentence1.similarity(sentence2)\n",
    "print(\"Similarity:\", similarity)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
