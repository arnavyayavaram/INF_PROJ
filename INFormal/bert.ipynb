{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial Experiment to test complexity of BERT Embeddings by comparing Cosine Similarity and Euclidian distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, TFBertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(model_name, max_seq_length):\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    input_word_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32)\n",
    "    input_mask = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32)\n",
    "    input_type_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32)\n",
    "    \n",
    "    inputs = {\n",
    "        'input_ids': input_word_ids,\n",
    "        'attention_mask': input_mask,\n",
    "        'token_type_ids': input_type_ids\n",
    "    }\n",
    "    \n",
    "    bert_layer = TFBertModel.from_pretrained(model_name)\n",
    "    outputs = bert_layer(inputs)\n",
    "    \n",
    "    pooled_output = outputs[1]  # Pooled output corresponds to the [CLS] token\n",
    "    return tf.keras.Model(inputs=[input_word_ids, input_mask, input_type_ids], outputs=pooled_output), tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# Function call\n",
    "max_seq_length = 128\n",
    "model_name = \"bert-base-uncased\"\n",
    "\n",
    "bert_model, tokenizer = get_model(model_name=model_name, max_seq_length=max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(input_text, model, tokenizer, max_seq_length):\n",
    "    input_ids = tokenizer.encode(input_text, add_special_tokens=True, max_length=max_seq_length, padding='max_length', truncation=True, return_tensors=\"tf\")\n",
    "    attention_mask = tf.ones_like(input_ids)\n",
    "    token_type_ids = tf.zeros_like(input_ids)\n",
    "    return model([input_ids, attention_mask, token_type_ids])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(embeddings, x, y):\n",
    "    dst = np.linalg.norm(embeddings[x] - embeddings[y])\n",
    "    return dst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(embeddings, x, y):\n",
    "    sim = 1 - distance.cosine(embeddings[x], embeddings[y])\n",
    "    return sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "real = \"dont worry about it\"\n",
    "idiom = \"dont sweat it\"\n",
    "sentences = [real, idiom,\"dont perspiration it\"]\n",
    "\n",
    "# Encode the sentences\n",
    "embeddings = [encode(sentence, bert_model, tokenizer, max_seq_length).numpy() for sentence in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Euclidean Distance between 'dont worry about it' & 'dont sweat it' is 1.4594606161117554\n",
      "Cosine Similarity between 'dont worry about it' & 'dont sweat it' is 0.9968292713165283\n",
      "Cosine Distance between 'dont worry about it' & 'dont sweat it' is 0.0031707286834716797\n",
      "\n",
      "Euclidean Distance between 'dont worry about it' & 'dont perspiration it' is 0.8992570638656616\n",
      "Cosine Similarity between 'dont worry about it' & 'dont perspiration it' is 0.9984480738639832\n",
      "Cosine Distance between 'dont worry about it' & 'dont perspiration it' is 0.0015519261360168457\n",
      "\n",
      "Euclidean Distance between 'dont sweat it' & 'dont perspiration it' is 1.212147831916809\n",
      "Cosine Similarity between 'dont sweat it' & 'dont perspiration it' is 0.9975985288619995\n",
      "Cosine Distance between 'dont sweat it' & 'dont perspiration it' is 0.0024014711380004883\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate and print distances\n",
    "for i in range(len(sentences)):\n",
    "    for j in range(i + 1, len(sentences)):\n",
    "        euclidean_dist = euclidean_distance(embeddings, i, j)\n",
    "        cosine_sim = cosine_similarity(embeddings, i, j)\n",
    "        cosine_dst=1-cosine_sim\n",
    "        print(\"Euclidean Distance between '{}' & '{}' is {}\".format(sentences[i], sentences[j], euclidean_dist))\n",
    "        print(\"Cosine Similarity between '{}' & '{}' is {}\".format(sentences[i], sentences[j], cosine_sim))\n",
    "        print(\"Cosine Distance between '{}' & '{}' is {}\".format(sentences[i], sentences[j], cosine_dst))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nlpaug.augmenter.word as naw\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nlpaug.augmenter.word as naw\n",
    "import nltk\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_random(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    words = [token.text for token in doc if token.pos_ in ['VERB','ADJ','NOUN']]\n",
    "    if(len(words)==0):\n",
    "        return doc[0].text\n",
    "    random_word_index = random.randrange(len(words))  # Generate a random index\n",
    "    random_word = words[random_word_index]\n",
    "    return random_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\siddh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\siddh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augmentation using SynonymAug\n",
    "aug = naw.SynonymAug(\n",
    "    aug_src='wordnet',\n",
    "    model_path=None,\n",
    "    name='Synonym_Aug',\n",
    "    aug_min=1,\n",
    "    aug_max=10,\n",
    "    aug_p=0.1,\n",
    "    lang='eng',\n",
    "    stopwords=None,\n",
    "    tokenizer=None,\n",
    "    reverse_tokenizer=None,\n",
    "    stopwords_regex=None,\n",
    "    force_reload=False,\n",
    "    verbose=0\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
